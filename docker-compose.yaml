########################################################################
# Root "/docker-compose.yaml"
########################################################################
name: thatdamtoolbox


########################################################################
# Network Bridge "damnet"
########################################################################
networks:
  damnet:
    driver: bridge


services:
  ########################################################################
  # 0. one-shot host configurator (runs only with `--profile setup`)
  ########################################################################
  hotspot-installer:
    image: thatdam-hotspot-installer:latest     # the image with setup.sh
    privileged: true                            # needs NET_ADMIN, etc.
    network_mode: host                          # touches wlan0 directly
    profiles: ["setup"]                         # ← won’t start in the main stack
    environment:
      SETUP_TOKEN_FILE: /run/secrets/dam_token  # setup.sh picks it up
    secrets:
      - source: dam_ephemeral_token
        target: dam_token                       # → /run/secrets/dam_token
    restart: "no"                               # it exits after writing configs

  ########################################################################
  # 1. public front-door (owns :80 / :443 on the Pi)
  ########################################################################
  gw:
    build:
      context: ./docker/nginx
      dockerfile: Dockerfile
    container_name: thatdamtoolbox-gateway
    network_mode: host             # owns :80 / :443 on the Pi
    # profiles: ["gateway"]
    restart: unless-stopped
    depends_on:
      - video-api
      - video-web

    # If you choose bridge mode instead, comment network_mode and add:
    # networks: [damnet]
    # ports: ["80:80"]

    environment:
      # host-net → talk to services via 127.0.0.1
      API_HOST: "127.0.0.1"
      API_PORT: "8080"
      WEB_HOST: "127.0.0.1"
      WEB_PORT: "3000"

    volumes:
      # allow you to override nginx.tmpl without rebuilding
      - ./docker/nginx/nginx.tmpl:/etc/nginx/nginx.tmpl:ro
      - ./docker/web-app/build:/usr/share/nginx/html:ro
    logging:
      driver: json-file
      options: { max-size: "10m", max-file: "3" }
      
  ########################################################################
  # 2. infrastructure services
  ########################################################################
  rabbitmq:
    #image: rabbitmq:3.13-alpine
    build:
      context: .
      dockerfile: docker/rabbitmq/Dockerfile
    image: cdaprod/thatdamtoolbox-rabbitmq   # was rabbitmq:3.13-alpine
    container_name: thatdamtoolbox-rabbitmq
    restart: unless-stopped
    networks: [damnet]
    hostname: rabbitmq
    environment:
      RABBITMQ_DEFAULT_USER: video
      RABBITMQ_DEFAULT_PASS: video
      RABBITMQ_DEFAULT_VHOST: /
    ports: ["5672:5672", "15672:15672"]   # 15672 optional (management UI)
    volumes:
      - rabbitmq_data:/var/lib/rabbitmq
    healthcheck:
      test: ["CMD", "rabbitmq-diagnostics", "ping"]
      interval: 10s
      timeout: 5s
      retries: 5


  ########################################################################
  # 3. capture daemon (needs /dev + v4l2, so bridge is fine)
  ########################################################################
  capture-daemon:
    build:
      context: .
      dockerfile: host/services/capture-daemon/Dockerfile
    image: cdaprod/video-capture-daemon:latest    # Can omit for local dev, keep for registry pushes
    container_name: thatdamtoolbox-capture-daemon
    networks: [damnet]
    init: false
    privileged: true                        # Needed for /dev and v4l2
    pid: host
    environment:
     AMQP_URL: "amqp://video:video@rabbitmq:5672/"
     REGISTRY_URL: "docker.io/cdaprod/capture-daemon:latest"  # Or your actual registry/repo URL
     CAPTURE_OUTDIR: "/records"
    volumes:
      - /dev:/dev
      - /lib/modules:/lib/modules:ro
      - ./data/modules/hwcapture/records:/records   # Adjust to your persistent record dir
    healthcheck:
      test: ["CMD", "curl", "-sf", "http://localhost:9000/devices"]
      interval: 10s
      timeout: 3s
      retries: 3
    restart: unless-stopped
    depends_on:
      - rabbitmq


  ########################################################################
  # 4. FastAPI back-end (mapped => reachable on host :8080)
  ########################################################################
  video-api:
    build:
      context: .
      dockerfile: Dockerfile            # Multi-arch build
    image: cdaprod/video:dev
    container_name: thatdamtoolbox-video-api
    platform: linux/arm64               # Pi 5; drop for x86-64 dev
    # --- Networking (host-mode exposes 8080) ---
    # (Depriciated) network_mode: host
    networks: [damnet]
    ports: ["8080:8080"]              # Unneeded in host mode
    # --- Environment ---------------------------
    # privileged: true                  # depriciated for security compliance
    user: "1000:1000"
    # --- Device permissions & hotplug ----------
    devices: []
      #- /dev/video0:/dev/video0
      #- /dev/video1:/dev/video1
      #- /dev/vchiq:/dev/vchiq
      #- /dev/dri:/dev/dri               # DRM for encoders/GL
    device_cgroup_rules:
      - "c 81:* rmw"                    # hot-plug /dev/video*
    group_add:
      - "44"                            # video group (GID 44 on Pi OS)
    cap_add:
      - SYS_ADMIN                        # only if v4l2 needs ioctl that’s otherwise blocked
    environment:
      TZ:               "America/New_York"
      VIDEO_MODE:       "api"
      PYTHONWARNINGS:   >-
            ignore::DeprecationWarning,
            ignore::UserWarning:google.protobuf.runtime_version
      # __cache location so DAM can write__
      XDG_CACHE_HOME:   /data/cache
      HF_HOME:          /data/cache/huggingface
      TORCH_HOME:       /data/cache/torch
      UVICORN_WORKERS:        "1"       # dev: 1  •  prod: 4
      VIDEO_FORCE_STDHTTP:    "0"       # 1: Force stdlib HTTP (optional)
      # ------------------------------------------------------------------
      # Make ALL mutable data live under one bind-mount ( /data )
      # ------------------------------------------------------------------
      
      # __override any others if you want–but everything else will fall back into $VIDEO_DATA_DIR:__
      VIDEO_DATA_DIR:         /data
      VIDEO_MEDIA_ROOT:       /data/media
      VIDEO_PROCESSED_ROOT:   /data/_PROCESSED
      VIDEO_PREVIEW_ROOT:     /data/previews
      VIDEO_LOG_DIR:          /data/logs
      
      # ── WAL-only DB lives here (on local volume) ──
      VIDEO_DB_PATH:          /var/lib/thatdamtoolbox/db/live.sqlite3
      DB_SNAPSHOT_SECS:       "120"
      VIDEO_STORAGE:          "auto"
      VIDEO_DEBUG_BOOT:       "0"
      
      # --- Storage backend selector -------------
      # VIDEO_STORAGE=sqlite                      # Default (no env needed)
      # VIDEO_STORAGE=faiss                       # Use Faiss vector index
      # VIDEO_STORAGE=weaviate                    # Use Weaviate backend
      # WEAVIATE_URL=http://localhost:8081        # For Weaviate backend:
      # MINIO_ENDPOINT: "http://localhost:9000"   # Optional if your code needs it
      EVENT_BROKER_URL: "amqp://video:video@rabbitmq:5672/"
      CAPTURE_REGISTRY_URL: "http://capture-daemon:9000"
    # --- Storage mounts ------------------------
    volumes:
      # __ 1) your SMB-mounted data (media, previews, etc.)__
      # (Depricated for below)- /mnt/b/Video/thatdamtoolbox:/data:rw
      - "${DATA_ROOT:-./data}:/data:rw"
      # __2) incoming staging dir__
      # (Depricated) - /mnt/b/Video/_INCOMING:/data/_INCOMING:rw
      - "${INCOMING_ROOT:-./incoming}:/data/_INCOMING:rw"
      # __3) local-only WAL DB__
      - db_wal:/var/lib/thatdamtoolbox/db:rw
      # __4) Live source for hot-reload (dev only)__
      - /dev:/dev:rw,rshared
      - ./video:/video:rw
    restart: unless-stopped
    depends_on:
      - capture-daemon
    # --- Entrypoint & Health -------------------
    # entrypoint: ["python", "-m", "video"]
    # command: [stats]                         # CLI command (optional)
    command: ["serve", "--host", "0.0.0.0", "--port", "8080"]
    mem_limit: 1500m
    cpus: 1.0
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:8080/health || exit 1"]
      interval: 60s
      timeout: 5s
      retries: 3

  ########################################################################
  # 5. Next.js dev server (exposed only for dev)
  ########################################################################
  video-web:
    build:
      context: ./docker/web-app
      dockerfile: Dockerfile
      target: development
    image: cdaprod/video-web:dev
    container_name: thatdamtoolbox-video-web-app
    platform: linux/arm64
    ports:
      - "3000:3000"
    # ------------------ (DEPRICATED) MAGIC HACK ------------------
    # let "host.docker.internal" point at your host (where video-api is in host mode)
    #extra_hosts:
      #- "host.docker.internal:host-gateway"
    networks: [damnet]
    environment:
      NODE_ENV: development
      # ← change these to talk to host.docker.internal
      #NEXT_PUBLIC_API_BASE_URL: "http://host.docker.internal:8080"
      #NEXT_PUBLIC_WS_URL:       "ws://host.docker.internal:8080/ws/camera"
      NEXT_PUBLIC_API_BASE_URL: "http://video-api:8080"
      NEXT_PUBLIC_WS_URL: "ws://video-api:8080/ws/camera"
      WATCHPACK_POLLING: "true"
      CHOKIDAR_USEPOLLING: "true"
    volumes:
      - ./docker/web-app:/app:rw
      #- ./docker/web-app/node_modules:/app/node_modules:rw
      #- ./docker/web-app/.next:/app/.next
      - /app/node_modules           # Anonymous volume, container owns it
      - /app/.next                  # Anonymous volume, container owns it
    depends_on:
      - video-api
    restart: unless-stopped
    mem_limit: 512m
    cpus: 0.5
    healthcheck:
      test: ["CMD-SHELL", "curl -fs http://localhost:3000/api/health || exit 1"]
      interval: 30s
      timeout: 5s
      retries: 3


  ########################################################################
  # 6. optional CLI job
  ########################################################################
  video-cli:
    image: cdaprod/video:dev
    container_name: thatdamtoolbox-video-cli-ephemeral
    networks: [damnet]
    profiles: [hydrate-backend, cli]
    entrypoint: ["python", "-m", "video"]
    # one-off scan job
    command: ["scan", "--root", "/data/_INCOMING", "--workers", "1"]
    # same mounts as video-api
    volumes:
      - /mnt/b/Video/thatdamtoolbox:/data:rw
      - /mnt/b/Video/_INCOMING:/data/_INCOMING:rw
      - db_wal:/var/lib/thatdamtoolbox/db:rw
    # do not restart this service automatically
    restart: "no"
    depends_on:
      - video-api

########################################################################
# container mounted volumes
########################################################################
volumes:
  db_wal:
    driver: local
  rabbitmq_data: